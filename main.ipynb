{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('ml': conda)",
   "display_name": "Python 3.7.9 64-bit ('ml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b4c961acfaebde5beef2a3bb1e97c2e556f2db98ea848dc13fe129e79b2a384b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preproc import create_dataset\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# StandardScaler because it scales the data to unit variance, which is good for this data since there's age data into it and StandardScaler makes all data into distrib of mean 0 unit variance => remove much of the magnitude from consideration (it improves the model by 2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = create_dataset(\n",
    "    1,\n",
    "    1,\n",
    "    test_size=0.4,\n",
    "    return_tensors=False\n",
    ")"
   ]
  },
  {
   "source": [
    "<h1>FIT EVAL FUNCTION</h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval(model, name):\n",
    "  model.fit(X_train, y_train)\n",
    "  preds1 = model.predict(X_train)\n",
    "  preds = [round(val) for val in preds1]\n",
    "  print('Accuracy on training data of', name, 'using %.3f' % (accuracy_score(preds, y_train)))\n",
    "  # on new data\n",
    "  test_preds = model.predict(X_test)\n",
    "  print('Accuracy on training data of', name, 'using %.3f' % (accuracy_score(test_preds, y_test)))\n",
    "  print(f'Confusion matrix on test data with {name}: \\n', confusion_matrix(test_preds, y_test))"
   ]
  },
  {
   "source": [
    "<h1>BUILDING THE MODELS</h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "models = dict()\n",
    "\n",
    "lr = LogisticRegression(\n",
    "  C=0.05,\n",
    "  penalty='l2'\n",
    ")\n",
    "lr_pipe = make_pipeline(StandardScaler(), lr)\n",
    "models['LogRes'] = lr_pipe\n",
    "\n",
    "### RF grossly overfits\n",
    "#XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "xgbc = XGBClassifier(\n",
    "  n_estimators=5, max_depth=5, \n",
    "  gamma=0.1\n",
    ")\n",
    "models['XGBC'] = xgbc\n",
    "\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "svc = make_pipeline(\n",
    "  StandardScaler(),\n",
    "  SVC(gamma=0.01, kernel='linear', probability=True)\n",
    ")\n",
    "models['SVC'] = svc\n",
    "## Using Tree classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## Random Forest:\n",
    "rf = RandomForestClassifier(\n",
    "  n_estimators=3,\n",
    "  max_depth=4, \n",
    "  criterion='gini',\n",
    ")\n",
    "models['RF'] = rf\n",
    "\n",
    "#Naive Bayes\n",
    "'''\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "models['GNB'] = gnb\n",
    "'''\n",
    "\n",
    "# K Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier(5)\n",
    "models['KNC']=knc"
   ]
  },
  {
   "source": [
    "<h1>FIT AND EVAL THE MODELS</h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy on training data of LogRes using 0.745\n",
      "Accuracy on training data of LogRes using 0.721\n",
      "Confusion matrix on test data with LogRes: \n",
      " [[188  75]\n",
      " [ 47 128]]\n",
      "Accuracy on training data of XGBC using 0.803\n",
      "Accuracy on training data of XGBC using 0.731\n",
      "Confusion matrix on test data with XGBC: \n",
      " [[195  78]\n",
      " [ 40 125]]\n",
      "Accuracy on training data of SVC using 0.738\n",
      "Accuracy on training data of SVC using 0.731\n",
      "Confusion matrix on test data with SVC: \n",
      " [[185  68]\n",
      " [ 50 135]]\n",
      "Accuracy on training data of RF using 0.745\n",
      "Accuracy on training data of RF using 0.715\n",
      "Confusion matrix on test data with RF: \n",
      " [[195  85]\n",
      " [ 40 118]]\n",
      "Accuracy on training data of KNC using 0.780\n",
      "Accuracy on training data of KNC using 0.685\n",
      "Confusion matrix on test data with KNC: \n",
      " [[180  83]\n",
      " [ 55 120]]\n"
     ]
    }
   ],
   "source": [
    "for name in models.keys():\n",
    "    fit_and_eval(models[name], name)"
   ]
  },
  {
   "source": [
    "Model is bad at predicting if people don't have heart disease in general ==> we need to regularize more and not let them predict harshly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross val accuracy: 0.729 (0.056) \n"
     ]
    }
   ],
   "source": [
    "#### K fold cross validation\n",
    "from numpy import mean, std\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "scores = cross_val_score(\n",
    "  svc, X_test, y_test,\n",
    "  scoring='accuracy', cv=cv, n_jobs=-1\n",
    ")\n",
    "print('Cross val accuracy: %.3f (%.3f) ' % (mean(scores), std(scores)))"
   ]
  },
  {
   "source": [
    "<h1>Stacked Classifier</h1>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy on training data of StackedModel using 0.799\nAccuracy on training data of StackedModel using 0.747\nConfusion matrix on test data with StackedModel: \n [[194  70]\n [ 41 133]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "all_models = [*models.items()]\n",
    "stacking = StackingClassifier(estimators=all_models, final_estimator=LogisticRegression())\n",
    "fit_and_eval(stacking, \"StackedModel\")"
   ]
  },
  {
   "source": [
    "**Conclusion**\n",
    "+ Stacked model is better than all other model by 1.4% to the best individual SVC \n",
    "- Overfits more and its precision is suffering (overpredicts ppl with no actual disease as having it)\n",
    "+ Predicts ppl with heart disease better than ppl without it => useful because we want real ppl to be cautious of it anyways, and there's no harm with ppl being wrongly diagnosed with it because there's not much risk.\n",
    "- However, around **<10%** of the diagnosis is risky (41 / 438 ppl to be false negatives i.e they actually have it but we say they don't). We want to *reduce this amount of false no-disease diagnoses*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}